\section{Language Transforms\timeestimation{25h}} % (fold)
\label{sec:transforms}
In this chapter, we'll discuss how we can use the notion of a data 
representation of a program to get a standard toolchain.
\subsection{Language Subsets} % (fold)
\label{sub:Language Subsets}
\begin{defn}
	Let {\tt A} and {\tt B} be two languages such that each valid {\tt B} 
	program is also a valid {\tt A} program. Further 
	$\forall b\in B\forall d\in Dat(B): \interpret{b}_A(d) = \interpret{b}_B(d)$

	Then $B$ is a language subset of $A$ (and conversely $A$ is a superset of
	$B$), writen $B \subset A$.
\end{defn}
\paragraph{{\tt C++}  and {\tt C} } % (fold)
\label{par:Cpp and C}
{\tt C++} was designed to be a object-oriented superset to the popular {\tt C}
programming language, so that the new {\tt C++} code could use legacy {\tt C}
code without modification. This notion was important to raise the acceptance of
{\tt C++} with programmers and eased switching.
% paragraph Cpp and C (end)
% subsection Language Subsets (end)
\subsection{Interpreter} % (fold)
\label{sub:Interpreter}

% subsection Interpreter (end)
\subsection{Compiler} % (fold)
\label{sub:Compiler}
\lineofthought{
	\begin{itemize}
		\item Simple definition: language transformer.
		\item 3 stages: frontend, independent middle and backend.
	\end{itemize}
}
\paragraph{How the {\tt gcc} is ported} % (fold)
\label{par:gcc}
When a new machine architecture is build, there is the problem that there is 
not yet any compiler for it. The na√Øve solution would be to write a complete 
compiler in the new machine language, but that would be very cumbersome and 
inefficient to do that for every new processor build.

There are two parts to the problem: on one hand, there is not any compiler, 
that outputs the new machine lanugage and on the other hand, there is no 
compiler that runs on the new machines.

For the first problem, we see that of the three stages of a modern compiler,
only the backend really depends on the output language. Often the backend has 
a general variant that can be parametrized for many architectures.
\footnote{For the whole process of writing a {\tt gcc} backend, see \cite{nilsson2000porting}}

The second problem is nowadays solved by cross-compiling -- when the language 
the compiler is executed in and the output language differ. Another approach 
was to have a minimal (non-optimizing) compiler or an interpreter to do the 
first translation.
%Instead the Gnu Compiler Collection uses a concept called {\em bootstrapping}:
%A very small subset of the languages supported by the {\em gcc} has its 
%compiler written in the machine language and this minimal compiler is then 
%used to compile the next compiler and so on, until the whole collection is 
%translated into the new machine language. 


\lineofthought{ Bootstrapping von {\tt gcc} }
% paragraph How the gcc is ported (end)

% subsection Compiler (end)
\subsection{Futamura Projections} % (fold)
\label{sub:Futamura}
\subsubsection{Specializer} % (fold)
\label{ssub:Specializer}
\subsubsection{Futamura Projections} % (fold)
\label{ssub:Futamura Projections}
The notion of a specializer as a transformer of source code has lead to some 
interesting observations: \lineofthought{ 
	\begin{itemize}
		\item An interpreter spec'ed with source is executable ($\rightarrow$ py2exe)
		\item A compiler is a specialized specializer with the step above.
		\item Repeat to get a compiler generator.
	\end{itemize} 
	Use types to visualize (
	$\applied{spec}: Input_1 \rightarrow \coded{\left( Input_2 \rightarrow Output \right)}$)
}

% subsubsection Futamura Projections (end)
\paragraph{The PyPy project} % (fold)
\label{par:The PyPy project}
\begin{example}
	The {\tt PyPy} project is an attempt to implement the popular {\tt
	Python}\footnote{\url{http://python.org/}} itself in a subset of {\tt Python}
	(called {\tt RPython}). Since {\tt Python} is an interpreted language, it
	would seem that this approach would lead to very slow execution, but that is
	not the case: PyPy uses Just-In-Time (JIT) specialization and compilation
	techniques in part described in \cite{psycho}.

	While the approach described in \ref{ssub:Specializer} is understood to be 
	executed before the actual program is run, it is also possible to run it 
	in parallel to the actual computation: Now the specializer can use 
	statistical information on the values. For example, while it might not be 
	obvious from the source that a certain value is constant and therefore a 
	static specializer might fail to set in, but a dynamic specializer can 
	determine this and produce a specialized function to call.

	For a highly dynamic language like {\tt Python}, it can lead to a hundredfold 
	speedup for very repetitive arithmetics\footnote{\cite{psycho}}.
\end{example}

% paragraph The PyPy project (end)

% subsubsection Specializer (end)
% subsection Futamura (end)
