As we saw, not all problems are solvable, but it seems not too far-fetched to
say that a problem not being solvable by any algorithm is seldom a concern for
most applications. Being solvable in a reasonable amount of time and space
however can very easily become a problem. What use is a program that solves our
problem, but takes hundreds of years to complete for any reasonable input? This
is where the field of complexity theory sets in: it characterises the solvable
problems by their "difficulty".

The most intuitive measure of difficulty is arguably time-complexity, i.e.\ how
long any algorithm will take to solve the problem in the worst case. Also
space-complexity needs to be considered, i.e.\ how much storage one needs to
complete the computation.

Since bigger problems are harder to solve and bigger problems need more data to
describe it fully, we analyse the asymptotic complexity in the size of the
input, i.e.\ how the time and space needed develop as the input grows towards
infinity. The most important distinctions here will be if there is a linear
relation or a polynomial of higher order -- or something worse, e.g.\ exponential
time/space.

Another important classification is that of non-determinism: if we could guess
during the computation, how would that affect the complexity of the problem?
Could we solve things faster, if we only guessed good enough? Surprisingly the
answer is that we are not sure (even though it is strongly suspected that being
a good guesser really helps).

As in computability with the Church's thesis, there is the question of what is
intuitively computable in a certain bound and even more than in computability,
this question seems to rely on the chosen formalism, but most of the
distinctions are quite robust. Still the question remains, if some
extraordinary feat of engineering can build a machine that is much faster than
our Turing machine model\footnote{Quantum computing is faster than \TM\@. A 
	quantum computer can search a list for an element in $O(\sqrt{n})$ time as 
	shown in \cite{grover1996fast}.}. 

The problems we discuss are decision problems:
\begin{defn}
	A problem $P$ is called a \emph{decision problem}, if the required output is 
	in $Bool = \{True, False\}$.
\end{defn}

Our high-level \WHILE language served us well in computability, but it would
be more confusing to measure time and space requirements than on the
lower-level Turing machine.

For example, how would we measure the used space in a simple, yet realistic
way: If I set {\tt Y := cons X X}, do I copy {\tt X} twice? Once?  Not at all?
What happens if I reassign a variable, that was used earlier?  This discussion
would surpass the scope of this text\footnote{\cite[p. 325f]{jones} gives a
possible measure.}.

\begin{defn}[Running time]
	For a machine $m\in \TM$ and the input $x$, we will write 
	$\measuretime[\TM]{m}(x)$ or just $\measuretime{m}(x)$ to denote the number 
	of steps that $m$ needs to get from the starting state $q_0$ to the end state 
	$e_{accept}$. If that does not happen, $\measuretime[\TM]{m}(x)=\infty$.
\end{defn}

%We will denote the time that a program $P\in \WHILE$ runs on an input $x$ by
%$\measuretime[\WHILE]{P}(x)$ or just $\measuretime{P}(x)$. To determine what
%that means, let's consider how long statements take to evaluate: we have only
%three kind of statements, assignment, if and while. Assigning a value can be
%thought of as writing an address, which can be done in one time-step. Of
%course, the right hand side might take some time to evaluate, so
%$\measuretime{X := E} = 1 + \measuretime{E}$. If statements take one step to
%check if the value is nil and the evaluation, likewise for while.  Also
%depending on the runtime values, the block its evaluated zero or more times.

%As for expressions, nil takes no time at all, since its not computed and all
%other expressions take one step plus the evaluation of subexpressions.
Not only time is a limited resource, also the memory that is used during the 
computation can limit the applications of an algorithm. Some problems could 
be solved for example just by reading the input and writing a fixed number of 
items into the memory -- for example searching an element in a list --, others
would need to increase their memory usage by a fixed amount each time the 
input doubles, for example calculating the median of a list of numbers, and 
some neen much more space.


\begin{defn}[space usage]
	The space usage $\measurespace{M}(x)$ for $M\in \TM$ is the maximum of the 
	maximum of the space usage of the states of its computation or $\infty$ if 
	the machine does not halt -- even if it looped on the same cells over and over.

	The space usage of a state of computation is the number of cells between the 
	left-most and the right-most non-blank cell. For example {\tt \dots \# \# 0 1 
	0 0 \# 1 0 1 1 \# \# \dots} would use 9 cells.
\end{defn}


\begin{defn}[Complexity Classes]
	\begin{equation*}
		\begin{split}
			\TIME[f]  &:= \{l\in \TM : \forall x\in \Input:  \measuretime{l}(x) \leq f(\abs{x})\} \\
			\SPACE[f] &:= \{l\in \TM : \forall x\in \Input: \measurespace{l}(x) \leq f(\abs{x})\}
		\end{split}
	\end{equation*}
	So
	\begin{enumerate}
		\item A program is in the complexity class $\TIME[f]$, if its running time is always bound 
			by $f$ of the size of the input.
		\item A program is in the complexity class $\SPACE[f]$, if its memory usage is always bound 
			by $f$ of the size of the input.
	\end{enumerate}
	And we call a problem $P$ in a complexity class $C$, if there is an 
	algorithm $p$ that solves it, such that $p\in C$.
\end{defn}

The most important complexity classes are those of the relatively slow 
growing polynomial functions:

\begin{defn}[Polynomial time and space]
	\begin{equation*}
		\begin{split}
			\PTIME &:= \bigcup_{p\text{ is polynomial}}\TIME[p] \\
			&= \left\{l\in \TM \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measuretime{l}(x)\leq p(\abs{x})\right\}\\
			\PSPACE &:= \bigcup_{p\text{ is polynomial}}\SPACE[p] \\
			&= \left\{l\in \TM \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measurespace{l}(x)\leq p(\abs{x})\right\} 
		\end{split}
	\end{equation*}

	That is:
	\begin{enumerate}
		\item A program is in the complexity class $\PTIME$, if its running time is always bound 
			by some polynomial.
		\item A program is in the complexity class $\PSPACE$, if its memory usage is always bound 
			by some polynomial.
	\end{enumerate}
\end{defn}

On the other hand the exponential functions grow so fast, that an algorithm 
taking exponentially long is often only feasible for small inputs.

\begin{defn}[Exponential time and space]
	\begin{equation*}
		\begin{split}
			\EXPTIME &:= \bigcup_{p\text{ is polynomial}} \TIME[2^{p(x)}]\\
			\EXPSPACE &:= \bigcup_{p\text{ is polynomial}} \SPACE[2^{p(x)}]\\
		\end{split}
	\end{equation*}
	So here
	\begin{enumerate}
		\item A program is in the complexity class $\EXPTIME$, if its running time is always bound 
			by an exponential of some polynomial.
		\item A program is in the complexity class $\EXPSPACE$, if its memory usage is always bound 
		by an exponential of some polynomial.
	\end{enumerate}
\end{defn}

\section{The complexity hierarchy}
We can clearly see that $\LOGSPACE\subseteq \PSPACE\subseteq\EXPSPACE$ just because of 
the functions involved, but the $\TIME$ and $\SPACE$ 
hierarchy is in fact interleaved, as the following theorems will prove.
\begin{theorem}
	\[\TIME[f] \subseteq \SPACE[f]\]
\end{theorem}
\begin{proof}
	On our tape, the only way to increase the number of cells used is to write 
	something in an blank cell. That we can do at most once per step.
\end{proof}

\begin{theorem}
	There are $a>0$ and $q > 1$ and $\exp(x) := a\cdot q^x$ such that:
	\[\SPACE[f] \subseteq \TIME[\exp\circ f]\]
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item Assume that $P\in\SPACE[f(x)]$, then $\measurespace{P}(x)\leq f(\abs{x})\forall x$.
		\item Assume that $\abs{x}$ is fixed, then in how many ways can the 
			memory arranged to hold that property? Each cell can hold any of the 
			symbols of $\Gamma$, so there are $\abs{\Gamma}^{f(\abs{x})}$ ways to 
			arrange that. Multiply that with the number of states that are not
			end states and you get the number of steps after either the machine is in
			a configuration it has seen before or goes to a new configuration --
			necessarily an end state. If the Turing machine is in the same
			configuration, then it will necessarily act in precisely the same way 
			as before, get to the same configuration again and again, and therefore loop. 
		\item But since $\measurespace{P}(x)\leq f(\abs{x})$, 
			$\measurespace{P}(x) \neq \infty$ and therefore 
			\[\measuretime{P}(x)\leq (\abs{Q} - 2)\cdot\abs{\Gamma}^{f(\abs{x})}\]
	\end{enumerate}
\end{proof}

\begin{corollary}
	\[ %\LOGSPACE\subseteq
		\PTIME\subseteq\PSPACE\subseteq\EXPTIME\subseteq\EXPSPACE \]
\end{corollary}

Surprisingly, it is not known, if these inclusions are strict. It has not 
been proven yet, that $\PTIME\neq \PSPACE$ or that $\PSPACE\neq\EXPTIME$. 
Complexity theory is full of these uncertainties, even if it superficially 
mirrors computability theory, it has proven to be hard on much more basic
levels than computability has.

\subsection{Hierarchy Theorems}
While not much is known, there are two important \emph{hierarchy theorems}, 
which give proper inclusions. To formulate that, we need the notion of a \emph{
time constructible function}\/:

\begin{defn}
	A function $f$ is called \emph{time constructible}\/ if there is a 
	$M\in\TM$, such that for sufficiently big $n$, $\TIME[M](1^n) = f(n)$.

	A function $f$ is called \emph{space constructible}\/ if there is a $M\in\TM$, 
	such that for sufficiently big $n$, $\SPACE[M](1^n)=f(n)$.

	Basically, this states, that there are Turing machines with this exact time 
	or space requirement $f(x)$.
\end{defn}
\begin{example}
	\begin{enumerate}
		\item $f(x)=x$ is time and space 
			constructible: Copy the input to the output tape.
		\item If $f$ is space constructible, then $g(x)=(x+1)\cdot f(x)$ time and space
			constructible. Run the machine from above on the output of the
			constructing machine: Running the constructing machine takes $f(x)$ time and space and then running the linear machine adds $x\cdot f(x)$.
		\item If $f$ is time/space constructible, then 
			$\forall c\in \mathbb{N^+}: g(x)=c\cdot f(x)$ is time/space 
			constructible: Run the constructing machine $M$ for $f$, but cicle through $c$ 
			states for each step of $M$.
		\item If $f$ and $g$ are time/space constructible, then $h(x) = f(x)+g(x)$ is time/space
			constructible. Just run them one after the other. 
		\item $f(x)=\sum^n_{k=0}c_k\cdot x^k$ is space and time constructible for 
			$c_i\in N^+$. Follows from above.
		\item $f(x)=2^x$ is time constructible: for the $k$th input symbol, count up 
			to the number $1(0)^k$ in binary.
		\item $f(x)=2^x$ is space constructible: First write $1$ on the output 
			tape, then for each element of the input, copy the output tape to its own end.
		\item If $f(x)\leq x$ for infinitely many $x$ then $f$ is \emph{not}\/ time
			constructible: We can't read the input, so we couldn't know, how long it
			would have been.
		\item This is not true for space constructible functions: We could even 
			read the input and not write anything, thus $f(x)=0$ is time constructible.
	\end{enumerate}
\end{example}

\begin{theorem}[Time Hierarchy]
	If $f$ is time-constructible, then
	\[ \TIME[o\left(f(x)\right)] \neq \TIME[f(x)^3] \]
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item The proof will show, that we can find a function, that is in 
			$\TIME[O(f(x)^3)]$, but not in any $\TIME[o\left(f(x)\right)]$
		\item The function that does that is 
			\[ \interpret{H_f}(M, n) \peq \begin{cases}
					True, &\text{if }\TIME[M](n) \leq f(\abs(n))\,\text{and $M$ ends in $q_{accept}$}\\
					False, &\text{otherwise}
			\end{cases} \]
			We can run $H_f$ by using an interpreter ($O(steps^2)$) and counting the steps on a 
			separate tape, so pessimistically\footnote{The Time Hierarchy Theorem 
			holds even for $\TIME[f(x)\cdot\log f(x)]$ with a trickier interpreter
			and counting} this is in $\TIME[O(f(x)^3]$. 
		\item On the other hand assume that $H_f\in \TIME[f(\floor{\frac{\abs{x}}{2}})]$, 
			then $\interpret{G}(m) :\peq not\,\interpret{H_f}(m, m)$ runs only
			\[f\left(\floor{\frac{2\abs{m}+1}{2}}\right)=f(\abs{m})\] steps. 
		\item Thus 
			\[\interpret{G}(G)\peq not\, \interpret{H_f}(G,G) \peq not\, True \peq False,\]
			but that would imply, that $H_f$ would reject it, therefore 
			\[\interpret{G}(G)\peq not\, \interpret{H_f}(G,G) \peq not\, False \peq True\]
			and vice versa -- which is a contradiction. Therefore $G\not\in\TIME{f(x)}$ 
			and $H_f\not\in\TIME{f\left(\floor{\frac{x}{2}}\right)}$
	\end{enumerate}
\end{proof}

\begin{corollary}
	\[\PTIME\neq\EXPTIME\]
\end{corollary}
\begin{proof}
	$x^n$ is time constructable for all $n\in \N$, but 
	$x^{3n} < 2^x \Leftrightarrow 3n\cdot \log_2 x < x \Leftrightarrow \frac{\log_2 x}{x} < 3n$
	, which holds for any $n$, if $x$ is big enough, so there is no polynomial 
	close enough to the exponentials, that would allow the \PTIME\/ class to 
	``jump'' into the \EXPTIME\/ class.
\end{proof}

\begin{theorem}[Space Hierarchy]
	If $f$ is space constructible, then $\SPACE[o(f(x))] \neq \SPACE[f(x)]$.
\end{theorem}
\begin{proof}
	This proof is very similar to the Time Hierarchy theorem. Instead of 
	counting the steps, we can write out $f(x)$ in unary on a second tape, and 
	move this in parallel to the computation tape. If at any point, an unmarked 
	cell is hit on the second tape, we know that we used more than $f(x)$ cells 
	and reject.
\end{proof}
This theorem is much stronger than the time hierarchy theorem, because it 
shows that any difference in the asymptotic behaviour leads to a different 
computability class.

\section{Nondeterminism} % (fold)
\label{sec:Nondeterminism}
Up until now, choice did not come into play at any moment: A program gave a 
unique trace of executed instructions for any given input. In many cases 
however, algorithms contain a moment of arbitrariness, where in the 
deterministic case, one fixed way would need to be chosen. The run-time of 
the algorithm can depend very much on the choices we make. 

\begin{defn}
	A non-deterministic Turing machine ($\NTM$) is a deterministic Turing machine, where 
	the transition function $\delta$ returns sets instead of values now.

	\[ \delta: Q\times \Gamma \rightarrow \mathcal{P}(Q\times \Gamma\times \{L, N, R\}) \]
\end{defn}

	The semantics need to adapt to that, a configuration of a non-deterministic 
	Turing machine needs to contain serveral configuration of a deterministic run:

\begin{defn}
	A \emph{configuration} is $C\subset \Gamma^*\times \Gamma \times \Gamma^*\times Q$
	and a step by a \NTM is $C_1\vdash C_2$ if 
	\[ C_2 = \bigcup_{c\in C_1}\set{ step(tape_c, s_c, t) : t\in \delta (c)} \]

	This allows us to define

	\[
		\interpret[\NTM]{M}(tape_1) = \bigcup_{(tape_1, q_0) \vdash^*_M C}\set{ tape : (tape, q_{accept}) \in C}
		\]

	So if any computation for some choices ends in $q_{accept}$, then the 
	computation of the non-deterministic machine succeeds. For decision 
	problems, we only need to know that this set is not empty.
\end{defn}

\begin{example}
	When parsing the string $\mathtt{aabab}$ with the regex 
	$\mathtt{(a|ab)+}$, we can't know after the first letter in which branch of 
	the  \emph{or} we will be. Only after we saw, that a second $\mathtt{a}$ 
	follows, we know that it was the first one.
\end{example}

\begin{defn}
	The time measure $\NTIME$ is the \emph{minimum} of the $\TIME$ measures of 
	the possible traces. Analogously, the space measure $\NSPACE$ is the 
	minimum of possible $\SPACE$ measures of the traces. It denotes the 
	fastest/least space consuming possible way to solve the problem, if guessing
	is allowed.
\end{defn}

At first, this seems like a new mode of computation and thus belong to
computability rather than complexity, but just proving that \emph{some}
computation accepts the input can be done by performing a breadth-first search
on the possible traces. The complexity however seems very different, because 
in the worst case, this approach will take $\mathcal{O}(2^{\mathcal{O}(n)})$ deterministic steps per non-deterministic step, thus an algorithm that takes $\NTIME f(x)$ will take $\TIME 2^{\mathcal{O}(f(x))}$ with this approach.

The most important class, that uses non-determinism is \NPTIME: The 
problems, that can be solved in polynomial time, if guessing is allowed and 
we guess optimally.

\begin{theorem}[Guess and check]
	If $X\in \NPTIME$, then we can find a so-called  \emph{certificate} 
	$w\in \Gamma^*$ for each input $x$, with $\abs{w} \leq p(\abs{x})$ for a polynomial $p$, so that there is a deterministic Turing 
	machine $M$ that decides $X$ given $w$ and $x$ in \PTIME. If we can 
	formulate the problem in such a way, then it is in \NPTIME.

	This means, that we can check our hypotheses for positive answers in
	polynomial time, so the hard part is only coming up with the good hypotheses.
	There is however no requirement for negative solutions, so the algorithm 
	might even fail by looping forever.\footnote{The class of decision problems, that 
	fail in non-deterministic polynomial time, but might run forever to check 
for success is called $co-\NPTIME$.}
\end{theorem}
\begin{proof}
	On one hand, we can code the trace, that lead to the succeeding computation, to the 
	tape. Checking, that a trace is valid for a given Turing machine is trivial. 

	If on the other hand we can produce a certificate for any input and check it in 
	polynomial time in $\abs{x}$ alone, then that certificate can only be $p(\abs{x})$ long for some 
	polynomial $p$ -- otherwise, we wouldn't be able to check the certificate this fast. 

	But then, we can define a non-deterministic machine, that comes up with 
	this input in $p(\abs{x})$ steps and checks that afterwards by simulating 
	the deterministic checker.
\end{proof}

We can see that $\PTIME\subset \NPTIME$, by just outputting singleton sets in 
the non-deterministic $\delta$ and intuitively it seems that non-determinism 
must accelerate the computation significantly.

\begin{thesis}
	$\PTIME\neq\NPTIME$
\end{thesis}

However, up to this point, every try to prove or disprove this statement failed.
