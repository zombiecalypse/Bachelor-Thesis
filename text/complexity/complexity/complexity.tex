As we saw, not all problems are solvable, but it seems not too farfetched to
say that a problem not being solvable by any algorithm is seldom a concern for
most applications. Being solvable in a reasonable amount of time and space
however can very easily become a problem. What use is a program that solves our
problem, but takes hundreds of years to complete for any reasonable input? This
is where the field of complexity theory sets in: it characterises the solvable
problems by their "difficulty".

The most intuitive measure of difficulty is arguably time-complexity, i.e.\ how
long any algorithm will take to solve the problem in the worst case. Also
space-complexity needs to be considered, i.e.\ how much storage one needs to
complete the computation.

Since bigger problems are harder to solve and bigger problems need more data to
describe it fully, we analyse the asymptotic complexity in the size of the
input, i.e.\ how the time and space needed develop as the input grows towards
infinity. The most important distinctions here will be if there is a linear
relation or a polynomial of higher order - or something worse, e.g.\ exponential
time/space.

Another important classification is that of non-determinism: if we could guess
during the computation, how would that affect the complexity of the problem?
Could we solve things faster, if we only guessed good enough? Surprisingly the
answer is that we are not sure (even though it is strongly suspected that being
a good guesser really helps).

As in computability with the Church's thesis, there is the question of what is
intuitively computable in a certain bound and even more than in computability,
this question seems to rely on the chosen formalism, but we will see that the
distinctions are quite robust. Still the question remains, if some
extraordinary feat of engineering can build a machine that is much faster than
our \WHILE language\footnote{Quantum computing is faster than \WHILE. A 
	quantum computer can search a list for an element in $O(\sqrt{n})$ time as 
	shown in \cite{grover1996fast}.}

\subsection{Time-complexity}
We will denote the time that a program $P\in \WHILE$ runs on an input $x$ by
$\measuretime[\WHILE]{P}(x)$ or just $\measuretime{P}(x)$. To determine what
that means, let's consider how long statements take to evaluate: we have only
three kind of statements, assignment, if and while. Assigning a value can be
thought of as writing an address, which can be done in one time-step. Of
course, the right hand side might take some time to evaluate, so
$\measuretime{X := E} = 1 + \measuretime{E}$. If statements take one step to
check if the value is nil and the evaluation, likewise for while.  Also
depending on the runtime values, the block its evaluated zero or more times.

As for expressions, nil takes no time at all, since its not computed and all
other expressions take one step plus the evaluation of subexpressions.

\subsection{Space-complexity}
Not only time is a limited resource, also the memory that is used during the 
computation can limit the applications of an algorithm. Some problems could 
be solved for example just by reading the input and writing a fixed number of 
items into the memory -- for example searching an element in a list --, others
would need to increase their memory usage by a fixed amount each time the 
input doubles, for example calculating the median of a list of numbers, and 
some neen much more space.

Unfortunately, the \WHILE language is not cut out to measure the used space 
in a simple, yet realistic way: If I set {\tt Y := cons X X}, do I copy {\tt X}
twice? Once?  Not at all? What happens if I reassign a variable, that was used
earlier?  This discussion would surpass the scope of this
text\footnote{\cite[p. 325f]{jones} gives a possible measure.}. So when 
talking about the space usage of a program $P$, we assume that it is compiled 
to a \TM and there we can measure the space usage quite comfortably:

The space usage $\measurespace{M}(x)$ for $M\in \TM$ is the maximum of the 
maximum of the space usage of the states of its computation.

The space usage of a state of computation is the number of cells between the 
left-most and the right-most non-blank cell. For example {\tt \dots \# \# 0 1 
0 0 0 1 0 1 1 \# \# \dots} would use 9 cells.

\begin{defn}[Complexity Classes]
	\begin{equation*}
		\begin{split}
			\TIME[f]  &:= \{l\in \WHILE : \forall x\in \Input:  \measuretime{l}(x) \leq f(\abs{x})\} \\
			\SPACE[f] &:= \{l\in \TM : \forall x\in \Input: \measurespace{l}(x) \leq f(\abs{x})\}
		\end{split}
	\end{equation*}
	So
	\begin{enumerate}
		\item A program is in the complexity class $\TIME[f]$, if its running time is always bound 
			by $f$ of the size of the input.
		\item A program is in the complexity class $\SPACE[f]$, if its memory usage is always bound 
			by $f$ of the size of the input.
	\end{enumerate}
	And we call a problem $P$ in a complexity class $C$, if there is an 
	algorithm $p$ that solves it, such that $p\in C$.
\end{defn}

The most important complexity classes are those of the relatively slow 
growing polynomial functions:

\begin{defn}[Polynomial time and space]
	\begin{equation*}
		\begin{split}
			\PTIME &:= \bigcup_{p\text{ is polynomial}}\TIME[p] \\
			&= \left\{l\in \WHILE \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measuretime{l}(x)\leq p(\abs{x})\right\}\\
			\PSPACE &:= \bigcup_{p\text{ is polynomial}}\SPACE[p] \\
			&= \left\{l\in \TM \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measurespace{l}(x)\leq p(\abs{x})\right\} 
		\end{split}
	\end{equation*}

	That is:
	\begin{enumerate}
		\item A program is in the complexity class $\PTIME$, if its running time is always bound 
			by some polynomial.
		\item A program is in the complexity class $\PSPACE$, if its memory usage is always bound 
			by some polynomial.
	\end{enumerate}
\end{defn}

On the other hand the exponential functions grow so fast, that an algorithm 
taking exponentially long is often only feasible for small inputs.

\begin{defn}[Exponential time and space]
	\begin{equation*}
		\begin{split}
			\EXPTIME &:= \bigcup_{p\text{ is polynomial}} \TIME[2^{p(x)}]\\
			\EXPSPACE &:= \bigcup_{p\text{ is polynomial}} \SPACE[2^{p(x)}]\\
		\end{split}
	\end{equation*}
	So here
	\begin{enumerate}
		\item A program is in the complexity class $\EXPTIME$, if its running time is always bound 
			by an exponential of some polynomial.
		\item A program is in the complexity class $\EXPSPACE$, if its memory usage is always bound 
		by an exponential of some polynomial.
	\end{enumerate}
\end{defn}
