As we saw, not all problems are solvable, but it seems not too farfetched to
say that a problem not being solvable by any algorithm is seldom a concern for
most applications. Being solvable in a reasonable amount of time and space
however can very easily become a problem. What use is a program that solves our
problem, but takes hundreds of years to complete for any reasonable input? This
is where the field of complexity theory sets in: it characterises the solvable
problems by their "difficulty".

The most intuitive measure of difficulty is arguably time-complexity, i.e.\ how
long any algorithm will take to solve the problem in the worst case. Also
space-complexity needs to be considered, i.e.\ how much storage one needs to
complete the computation.

Since bigger problems are harder to solve and bigger problems need more data to
describe it fully, we analyse the asymptotic complexity in the size of the
input, i.e.\ how the time and space needed develop as the input grows towards
infinity. The most important distinctions here will be if there is a linear
relation or a polynomial of higher order -- or something worse, e.g.\ exponential
time/space.

Another important classification is that of non-determinism: if we could guess
during the computation, how would that affect the complexity of the problem?
Could we solve things faster, if we only guessed good enough? Surprisingly the
answer is that we are not sure (even though it is strongly suspected that being
a good guesser really helps).

As in computability with the Church's thesis, there is the question of what is
intuitively computable in a certain bound and even more than in computability,
this question seems to rely on the chosen formalism, but most of the
distinctions are quite robust. Still the question remains, if some
extraordinary feat of engineering can build a machine that is much faster than
our Turing machine model\footnote{Quantum computing is faster than \TM\@. A 
	quantum computer can search a list for an element in $O(\sqrt{n})$ time as 
	shown in \cite{grover1996fast}.}. 

The problems we discuss are decision problems:
\begin{defn}
	A problem $P$ is called a {\em decision problem}, if the required output is 
	$Bool = \{True, False\}$.
\end{defn}

Our high-level \WHILE language served us well in computability, but it would
be more confusing to measure time and space requirements than on the
lower-level Turing machine.

For example, how would we measure the used space in a simple, yet realistic
way: If I set {\tt Y := cons X X}, do I copy {\tt X} twice? Once?  Not at all?
What happens if I reassign a variable, that was used earlier?  This discussion
would surpass the scope of this text\footnote{\cite[p. 325f]{jones} gives a
possible measure.}.

\begin{defn}[Running time]
	For a machine $m\in \TM$ and the input $x$, we will write 
	$\measuretime[\TM]{m}(x)$ or just $\measuretime{m}(x)$ to denote the number 
	of steps that $m$ needs to get from the starting state $q_0$ to the end state 
	$e_{accept}$. If that does not happen, $\measuretime[\TM]{m}(x)=\infty$.
\end{defn}

%We will denote the time that a program $P\in \WHILE$ runs on an input $x$ by
%$\measuretime[\WHILE]{P}(x)$ or just $\measuretime{P}(x)$. To determine what
%that means, let's consider how long statements take to evaluate: we have only
%three kind of statements, assignment, if and while. Assigning a value can be
%thought of as writing an address, which can be done in one time-step. Of
%course, the right hand side might take some time to evaluate, so
%$\measuretime{X := E} = 1 + \measuretime{E}$. If statements take one step to
%check if the value is nil and the evaluation, likewise for while.  Also
%depending on the runtime values, the block its evaluated zero or more times.

%As for expressions, nil takes no time at all, since its not computed and all
%other expressions take one step plus the evaluation of subexpressions.
Not only time is a limited resource, also the memory that is used during the 
computation can limit the applications of an algorithm. Some problems could 
be solved for example just by reading the input and writing a fixed number of 
items into the memory -- for example searching an element in a list --, others
would need to increase their memory usage by a fixed amount each time the 
input doubles, for example calculating the median of a list of numbers, and 
some neen much more space.


\begin{defn}[space usage]
	The space usage $\measurespace{M}(x)$ for $M\in \TM$ is the maximum of the 
	maximum of the space usage of the states of its computation or $\infty$ if 
	the machine does not halt -- even if it looped on the same cells over and over.

	The space usage of a state of computation is the number of cells between the 
	left-most and the right-most non-blank cell. For example {\tt \dots \# \# 0 1 
	0 0 \# 1 0 1 1 \# \# \dots} would use 9 cells.
\end{defn}


\begin{defn}[Complexity Classes]
	\begin{equation*}
		\begin{split}
			\TIME[f]  &:= \{l\in \TM : \forall x\in \Input:  \measuretime{l}(x) \leq f(\abs{x})\} \\
			\SPACE[f] &:= \{l\in \TM : \forall x\in \Input: \measurespace{l}(x) \leq f(\abs{x})\}
		\end{split}
	\end{equation*}
	So
	\begin{enumerate}
		\item A program is in the complexity class $\TIME[f]$, if its running time is always bound 
			by $f$ of the size of the input.
		\item A program is in the complexity class $\SPACE[f]$, if its memory usage is always bound 
			by $f$ of the size of the input.
	\end{enumerate}
	And we call a problem $P$ in a complexity class $C$, if there is an 
	algorithm $p$ that solves it, such that $p\in C$.
\end{defn}

The most important complexity classes are those of the relatively slow 
growing polynomial functions:

\begin{defn}[Polynomial time and space]
	\begin{equation*}
		\begin{split}
			\PTIME &:= \bigcup_{p\text{ is polynomial}}\TIME[p] \\
			&= \left\{l\in \TM \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measuretime{l}(x)\leq p(\abs{x})\right\}\\
			\PSPACE &:= \bigcup_{p\text{ is polynomial}}\SPACE[p] \\
			&= \left\{l\in \TM \exists p=\sum^n_{k=0}a_k\,x^k: \forall x\in \Input: \measurespace{l}(x)\leq p(\abs{x})\right\} 
		\end{split}
	\end{equation*}

	That is:
	\begin{enumerate}
		\item A program is in the complexity class $\PTIME$, if its running time is always bound 
			by some polynomial.
		\item A program is in the complexity class $\PSPACE$, if its memory usage is always bound 
			by some polynomial.
	\end{enumerate}
\end{defn}

On the other hand the exponential functions grow so fast, that an algorithm 
taking exponentially long is often only feasible for small inputs.

\begin{defn}[Exponential time and space]
	\begin{equation*}
		\begin{split}
			\EXPTIME &:= \bigcup_{p\text{ is polynomial}} \TIME[2^{p(x)}]\\
			\EXPSPACE &:= \bigcup_{p\text{ is polynomial}} \SPACE[2^{p(x)}]\\
		\end{split}
	\end{equation*}
	So here
	\begin{enumerate}
		\item A program is in the complexity class $\EXPTIME$, if its running time is always bound 
			by an exponential of some polynomial.
		\item A program is in the complexity class $\EXPSPACE$, if its memory usage is always bound 
		by an exponential of some polynomial.
	\end{enumerate}
\end{defn}

\section{The complexity hierarchy}
We can clearly see that $\LOGSPACE\subseteq \PSPACE\subseteq\EXPSPACE$ just because of 
the functions involved, but the $\TIME$ and $\SPACE$ 
hierarchy is in fact interleaved, as the following theorems will prove.
\begin{theorem}
	\[\TIME[f] \subseteq \SPACE[f]\]
\end{theorem}
\begin{proof}
	On our tape, the only way to increase the number of cells used is to write 
	something in an blank cell. That we can do at most once per step.
\end{proof}

\begin{theorem}
	There are $a>0$ and $q > 1$ and $\exp(x) := a\cdot q^x$ such that:
	\[\SPACE[f] \subseteq \TIME[\exp\circ f]\]
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item Assume that $P\in\SPACE[f(x)]$, then $\measurespace{P}(x)\leq f(\abs{x})\forall x$.
		\item Assume that $\abs{x}$ is fixed, then in how many ways can the 
			memory arranged to hold that property? Each cell can hold any of the 
			symbols of $\Gamma$, so there are $\abs{\Gamma}^{f(\abs{x})}$ ways to 
			arrange that. Multiply that with the number of states that are not
			endstates and you get the number of steps after either the machine is in
			a configuration it has seen before or goes to a new configuration --
			necessarily an endstate. If the Turing machine is in the same
			configuration, then it will necessarily act in precisely the same way 
			as before, get to the same configuration again and again, and therefore loop. 
		\item But since $\measurespace{P}(x)\leq f(\abs{x})$, 
			$\measurespace{P}(x) \neq \infty$ and therefore 
			\[\measuretime{P}(x)\leq (\abs{Q} - 2)\cdot\abs{\Gamma}^{f(\abs{x})}\]
	\end{enumerate}
\end{proof}

\begin{corrolary}
	\[ %\LOGSPACE\subseteq
		\PTIME\subseteq\PSPACE\subseteq\EXPTIME\subseteq\EXPSPACE \]
\end{corrolary}

Surprisingly, it is not known, if these inclusions are strict. It has not 
been proven yet, that $\PTIME\neq \PSPACE$ or that $\PSPACE\neq\EXPTIME$. 
Complexity theory is full of these uncertainities, even if it superficially 
mirrors computability theory, it has proven to be hard on much more basic
levels than computability has.

\subsection{Hierarchy Theorems}
While not much is known, there are two important {\em hierarchy theorems}, 
which give proper inclusions. To formulate that, we need the notion of a {\em 
time constructible function}\/:

\begin{defn}
	A function $f$ is called {\em time constructible}\/ if there is a 
	$M\in\TM$, such that for sufficiently big $n$, $\TIME[M](1^n) = f(n)$.

	A function $f$ is called {\em space constructible}\/ if there is a $M\in\TM$, 
	such that for sufficiently big $n$, $\SPACE[M](1^n)=f(n)$.

	Basically, this states, that there are Turing machines with this exact time 
	or space requirement $f(x)$.
\end{defn}
\begin{example}
	\begin{enumerate}
		\item $f(x)=x$ is time and space 
			constructible: Copy the input to the output tape.
		\item If $f$ is space constructible, then $g(x)=(x+1)\cdot f(x)$ time and space
			constructible. Run the machine from above on the output (the $x$) of the
			constructing machine (the $+1$).
		\item If $f$ is time/space constructible, then 
			$\forall c\in \mathbb{N^+}: g(x)=c\cdot f(x)$ is time/space 
			constructible: Run the constructing machine $M$ for $f$, but cicle through $c$ 
			states for each step of $M$.
		\item If $f$ and $g$ are time/space constructible, then $h(x) = f(x)+g(x)$ is time/space
			constructible. Just run them one after the other. 
		\item $f(x)=\sum^n_{k=0}c_k\cdot x^k$ is space and time constructible for 
			$c_i\in N^+$. Follows from above.
		\item $f(x)=2^x$ is time constructible: for the $k$th input symbol, count up 
			to the number $1(0)^k$ in binary.
		\item $f(x)=2^x$ is space constructible: First write $1$ on the output 
			tape, then for each element of the input, copy the output tape to its own end.
		\item If $f(x)\leq x$ for infinitely many $x$ then $f$ is {\em not}\/ time
			constructible: We can't read the input, so we couldn't know, how long it
			would have been.
		\item This is not true for space constructible functions: We could even 
			read the input and not write anything, thus $f(x)=0$ is time constructible.
	\end{enumerate}
\end{example}

\begin{theorem}[Time Hierarchy]
	If $f$ is time-constructible, then
	\[ \TIME[o\left(f(x)\right)] \neq \TIME[f(x)^3] \]
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item The proof will show, that we can find a function, that is in 
			$\TIME[O(f(x)^3)]$, but not in any $\TIME[o\left(f(x)\right)]$
		\item The function that does that is 
			\[ \interpret{H_f}(M, n) \peq \begin{cases}
					True, &\text{if }\TIME[M](n) \leq f(\abs(n))\,\text{and $M$ ends in $q_{accept}$}\\
					False, &\text{otherwise}
			\end{cases} \]
			We can run $H_f$ by using an interpreter ($O(steps^2)$) and counting the steps on a 
			separate tape, so pessimistically\footnote{The Time Hierarchy Theorem 
			holds even for $\TIME[f(x)\cdot\log f(x)]$ with a trickier interpreter
			and counting} this is in $\TIME[O(f(x)^3]$. 
		\item On the other hand assume that $H_f\in \TIME[f(\floor{\frac{\abs{x}}{2}})]$, 
			then $\interpret{G}(m) :\peq not\,\interpret{H_f}(m, m)$ runs only
			\[f\left(\floor{\frac{2\abs{m}+1}{2}}\right)=f(\abs{m})\] steps. 
		\item Thus 
			\[\interpret{G}(G)\peq not\, \interpret{H_f}(G,G) \peq not\, True \peq False\]
			, but that would imply, that $H_f$ would reject it, therefore 
			\[\interpret{G}(G)\peq not\, \interpret{H_f}(G,G) \peq not\, False \peq True\]
			and vice versa -- which is a contradiction. Therefore $G\not\in\TIME{f(x)}$ 
			and $H_f\not\in\TIME{f\left(\floor{\frac{x}{2}}\right)}$
	\end{enumerate}
\end{proof}

\begin{corrolary}
	\[\PTIME\neq\EXPTIME\]
\end{corrolary}
\begin{proof}
	$x^n$ is time computable for all $n\in \N$, but 
	$x^{3n} < 2^x \Leftrightarrow 3n\cdot \log_2 x < x \Leftrightarrow \frac{\log_2 x}{x} < 3n$
	, which holds for any $n$, if $x$ is big enough, so there is no polynomial 
	close enough to the exponentials, that would allow the \PTIME\/ class to 
	``jump'' into the \EXPTIME\/ class.
\end{proof}

\begin{theorem}[Space Hierarchy]
	If $f$ is space constructible, then $\SPACE[o(f(x))] \neq \SPACE[f(x)]$.
\end{theorem}
\begin{proof}
	This proof is very similar to the Time Hierarchy theorem. Instead of 
	counting the steps, we can write out $f(x)$ in unary on a second tape, and 
	move this in parallel to the computation tape. If at any point, an unmarked 
	cell is hit on the second tape, we know that we used more than $f(x)$ cells 
	and reject.
\end{proof}
This theorem is much stronger than the time hierarchy theorem, because it 
shows that any difference in the asymptotic behaviour leads to a different 
computability class.
