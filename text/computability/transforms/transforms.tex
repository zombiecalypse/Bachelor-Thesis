\section{Language Transforms\timeestimation{25h}} % (fold)
\label{sec:transforms}
In this chapter, we'll discuss how we can use the notion of a data 
representation of a program to get a standard toolchain.
\subsection{Language Subsets} % (fold)
\label{sub:Language Subsets}
\begin{defn}
	Let {\tt A} and {\tt B} be two languages such that each valid {\tt B} 
	program is also a valid {\tt A} program. Further 
	$\forall b\in B\, \forall d\in Dat(B): \interpret[A]{b}(d) = \interpret[B]{b}(d)$

	Then $B$ is a language subset of $A$ (and conversely $A$ is a superset of
	$B$), writen $B \subset A$.
\end{defn}
\paragraph{{\tt C++}  and {\tt C} } % (fold)
\label{par:Cpp and C}
{\tt C++} was designed to be a object-oriented superset to the popular {\tt C}
programming language, so that the new {\tt C++} code could use legacy {\tt C}
code without modification. This notion was important to raise the acceptance of
{\tt C++} with programmers and eased switching.
% paragraph Cpp and C (end)
% subsection Language Subsets (end)
\subsection{Interpreter} % (fold)
\label{sub:Interpreter}
Today programmers and machines seldom speak the same language. Programming in 
machine language is difficult, error-prone and unportable to name only a few 
drawbacks. However it seems reasonable to expect to be able to execute ones 
code nonetheless. Typically, we want our computer to interpret what we mean 
in our high-level programming language. An {\em interpreter} is such a program.

\begin{defn}
	An {\em interpreter} of $A$ is a program $interp$ such that
	\begin{align*}
			\interpret[P]{interp}&: A \times \Input[A] \longrightarrow \Output[A] \\
			\interpret[P]{interp}&(a, d) = \interpret[A]{a}(d)
	\end{align*}
\end{defn}

An interpreter typically parses the program code to a Abstract Syntax Tree 
(AST), in which the relations of the statements can easily be infered. For 
example, the AST of

\begin{verbatim}
	unary_length read X {
	  Y := nil
	  FOR Z IN X {
	    Y := cons nil Y
	  }
	} write Y
\end{verbatim}

might look like

\begin{verbatim}
	(PROCEDURE unary_length X Y
	  (
	    (ASSIGN Y nil)
	    (FOR Z X
	      (ASSIGN Y (cons nil (VAR Y)))
	    )
	  )
	)
\end{verbatim}

Since parsing is not part of this text, we will assume, that a convenient 
format is already given.\footnote{If you are interested in the whole story, 
	\cite{aho2007compilers} offers a good introduction into the theory of
	building tools for a new language and the appendix
	\ref{sec:implementingWhile} explains how the implementation of \WHILE works.}


Interpreters are typically the first step in implementing a language: they 
are relatively easy to write and therefore allow experimenting. The downside 
is that working on the AST and constantly translating typically takes longer 
than an equivalent $A$ program would take. This {\em overhead} is often 
just a fixed factor, but that factor could be 100, making the interpreted program a 
hundred times slower than the native one.

% subsection Interpreter (end)
\subsection{Compiler} % (fold)
\label{sub:Compiler}
Given the problem of a language that can not be executed directly, there is 
also another approach that can be taken instead of interpretation: We could 
translate the program into a native one in the executable language. This 
process is called {\em compiling} and the program that does this is a {\em
compiler}.

\begin{defn}
	A {\em compiler} for the language $A$ into the language $B$ is a program
	$compile\in P$, such that
	\begin{align*}
		\interpret[P]{compile} &: A \longrightarrow B\\
		\interpret[B]{\interpret[P]{compile}(a)} &= \interpret[A]{a}
	\end{align*}

	Normally $P=B$, but if it is {\em not}, then $compile$ is a so-called {\em
	cross-compiler}.
\end{defn}

What does it mean, if we can express an compiler for $A$ in the language 
$B$? It means, that we can solve any problem in $B$ that can be solved in $A$, 
by compiling the $A$ solution to $B$. This allows us to classify languages 
in the following way.

\begin{defn}
	\label{def:power}
	The language $A$ is {\em at least as powerful} as $B$, if there is an 
	compiler $\Compiler{B}{A}\in A$. We write $A \lepower B$.

	Similarly, $A$ are {\em equally powerful} or {\em Turing equivalent} if $A$
	can compile $B$ and vice versa. We write $A \epower B$.
\end{defn}

\begin{theorem}
	\begin{enumerate}
		\item $\lepower$ is reflexive, that is $A\lepower A$.
		\item $\lepower$ is transitive, that is if $A\lepower B\lepower C$, then $A\lepower C$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item The compiler is the identity.
		\item \[\Compiler[C]{A}{B} = \interpret[A]{\Compiler{B}{C}}(\Compiler{A}{B})\]
			and then we can compose $\interpret[C]{\Compiler{B}{C}} \circ \interpret[C]{\Compiler[C]{A}{B}}$
	\end{enumerate}
\end{proof}

\subsubsection{How a modern compiler works}
A compiler typically has three stages, a frontend, a middle, and a backend.

The {\em frontend} transforms the language into a handy format, that is not 
necessarily similar to the input language. Compiler collections may have many 
frontends, that transform all kind of languages into this intermediate format.

The {\em middle} makes all kind of transformations on the immediate format, 
for example optimizations, type checking, \dots. This is where productive 
work can be done, so much work is typically invested here.

In the {\em backend}, the actual output is generated by transforming the 
immediate format to the output language. Separating this from the middle 
makes it easier to work for different output languages.

\paragraph{How the {\tt gcc} is ported} % (fold)
\label{par:gcc}
When a new machine architecture is build, there is the problem that there is 
not yet any compiler for it. The na√Øve solution would be to write a complete 
compiler in the new machine language, but that would be very cumbersome and 
inefficient to do that for every new processor build.

There are two parts to the problem: on one hand, there is not any compiler, 
that outputs the new machine lanugage and on the other hand, there is no 
compiler that runs on the new machines.

For the first problem, we see that of the three stages of a modern compiler,
only the backend really depends on the output language. Often the backend has 
a general variant that can be parametrized for many architectures.
\footnote{For the whole process of writing a {\tt gcc} backend, see \cite{nilsson2000porting}}

The second problem is nowadays solved by cross-compiling -- when the language 
the compiler is executed in and the output language differ. Another approach 
was to have a minimal (non-optimizing) compiler or an interpreter to do the 
first translation.

% paragraph How the gcc is ported (end)
% subsection Compiler (end)
\subsection{Interpretation versus compilation}

\subsubsection{Specializer} % (fold)
\label{sub:Specializer}
While programmers generally know the notions of a compiler and possibly of an 
interpreter, the {\em specializer} is less well known. {\em Specialization} 
is the process of fixing a certain value in the source code, even though it 
was written in a way that would have allowed different values. 

\begin{defn}
	A {\em specializer} $spec$ for the language $A$ is a program 
	\begin{align*}
		\interpret[P]{spec}&:A\times \Input[A]\rightarrow A \\
		\interpret[A]{\interpret[P]{spec}(a, x)}(y) &= \interpret[A]{a}(x.y)
	\end{align*}

	Informally speaking, the specializer moves an argument from the runtime to 
	the compile time.
\end{defn}

Typically, this is part of the optimizations that occur during compilation. For
example, if we encountered the call {\tt fib(n, true)} for the function

\begin{verbatim}
	int fib(int n, bool debug) {
	  if (debug) printf("Call with %d", n);

	 	if (n == 0 || n == 1) return 0;

	 	return fib(n-1) + fib(n-2);
	}
\end{verbatim}
then the specializer might remove the first test and just leave the {\tt 
printf}, which is more efficient and removes the ``loose end'' {\tt debug}  
from the runtime.

For a specializer to work, it has to prove, that a certain part of the 
program only depends on the given value and static data and then evaluate 
that. This sounds easier than it is: 
\begin{example}
	Given the expression {\tt a+b+1}, where {\tt a} is statically known to be $5$, 
	we know that it could be specialized to {\tt b+6}, but if we interpreted it 
	as {\tt (a+b)+1} {\em or} {\tt a+(b+1)}, no subexpression would be
	independent of {\tt b}.
\end{example}
For a deeper look at the applications and implementations of specializers, 
see \cite{jones1993partial}.


\subsubsection{Futamura Projections} % (fold)
\label{ssub:Futamura Projections}
The notion of a specializer as a transformer of source code has lead to some 
interesting observations by Yoshihiko Futamura
\footnote{\cite{futamura1999partial}}, which are now known as the Futamura
Projections:

\begin{enumerate}
	\item \begin{align*}
			\interpret[P]{\interpret[P]{spec}(\Interpreter{A}, source)}(inp)
			&= \interpret[P]{\Interpreter{A}}(source.inp) \\
			&= \interpret[A]{source}(inp)
	\end{align*}
		So we can get an executable, if we specialize the interpreter with the 
		source of our program.
	\item \begin{equation*}
			\begin{split}
			compiler_{A\rightarrow P} &= \interpret[P]{spec}(spec, \Interpreter{A}) \\
				\interpret[P]{\interpret[P]{spec}(spec, \Interpreter{A})}(source)
				&= \interpret[P]{spec}(\Interpreter{A}.source)
		\end{split}
	\end{equation*}
		Therefore we can get a compiler, if we specialize the specializer with the interpreter.
	\item \begin{align*}
			\interpret[P]{\interpret[P]{spec}(spec.spec)}(\Interpreter{A})
			&= \interpret[P]{spec}(spec.\Interpreter{A})
	\end{align*}
		So we can get a program, that takes an interpreter for any language $A$ 
		and produces a compiler for $A$ from it.
\end{enumerate}

This approach would make it as easy to generate a compiler as it is to 
program an interpreter, so why are not all compilers generated this way?

As discussed, finding specializable parts of the interpreter is not as easy 
as the equations make it look, so it is not surprising, that a correct 
specializer will not catch every possible optimization. In fact, optimizing 
the intermediate format allows many other optimizations besides the
specialization and so a pragmatic compiler writer will prefer to do such 
things by hand, instead of enhancing the specializer.

% subsubsection Futamura Projections (end)
\paragraph{The PyPy project} % (fold)
\label{par:ThePyPyproject}
\begin{example}
	The {\tt PyPy} project is an attempt to implement the popular {\tt
	Python}\footnote{\url{http://python.org/}} itself in a subset of {\tt Python}
	(called {\tt RPython}). Since {\tt Python} is an interpreted language, it
	would seem that this approach would lead to very slow execution, but that is
	not the case: PyPy uses Just-In-Time (JIT) specialization and compilation
	techniques in part described in \cite{psycho}.

	While the approach described in \ref{sub:Specializer} is understood to be 
	executed before the actual program is run, it is also possible to run it 
	in parallel to the actual computation: Now the specializer can use 
	statistical information on the values. For example, while it might not be 
	obvious from the source that a certain value is constant and therefore a 
	static specializer might fail to set in, but a dynamic specializer can 
	determine this and produce a specialized function to call.

	For a highly dynamic language like {\tt Python}, it can lead to a hundredfold 
	speedup for very repetitive arithmetics\footnote{\cite{psycho}}.
\end{example}

% paragraph The PyPy project (end)

\subsubsection{Theoretical Results}
\label{ssub:spec theo}
Proposing that we can statically fix arguments in our program leads important results:
\begin{theorem}
	\label{thm:power-interpreter}
	If there is an interpreter $\Interpreter{B}\in A$ then $B\leq A$.
\end{theorem}
\begin{proof}
	By the second Futamura projection 
	$\Compiler{B}{A} = \interpret[A]{spec}(spec, \Interpreter{B})$
\end{proof}

Note however that the reversal is not necessarily true: For example, we know, that 
\FOR is has no $\Interpreter{\FOR}$, but $\FOR \leq \FOR$ still holds. We 
will see later that a self-interpreter allows us to identify the two notions.
% subsection Specializer (end)
